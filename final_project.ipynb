{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f1bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Total Train Rows: 890216\n",
      "Total Test Rows:  79768\n",
      "Total Train Rows: 890216\n",
      "Total Test Rows:  79768\n",
      "\n",
      "[Processing Complete]\n",
      "Train Samples (Groups): 111277 (= 890216 rows)\n",
      "Test Samples (Groups):  9971 (= 79768 rows)\n",
      "Applying Advanced CSI Correction (Smoothing + Noise Removal)...\n",
      "\n",
      "[Processing Complete]\n",
      "Train Samples (Groups): 111277 (= 890216 rows)\n",
      "Test Samples (Groups):  9971 (= 79768 rows)\n",
      "Applying Advanced CSI Correction (Smoothing + Noise Removal)...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LeakyReLU \n",
    "\n",
    "\n",
    "# 데이터 로딩\n",
    "train_path = '/content/drive/MyDrive/Toa_sort.csv'\n",
    "test_path = '/content/drive/MyDrive/Toa_test_sort.csv'\n",
    "\n",
    "# 데이터 로딩 (헤더 없음)\n",
    "columns = ['re_x', 're_y', 'anchor_id', 'TOA'] + [f'CSI_{i}' for i in range(1, 129)]\n",
    "df_train = pd.read_csv(train_path, header=None, names=columns)\n",
    "df_test = pd.read_csv(test_path, header=None, names=columns)\n",
    "\n",
    "# 출력: re_x, re_y\n",
    "output_cols = ['re_x', 're_y']\n",
    "\n",
    "# 입력 특징: CSI_1 to CSI_128\n",
    "csi_cols = [f'CSI_{i}' for i in range(1, 129)]\n",
    "\n",
    "# --- 그룹화 방식: 연속 8행을 하나의 입력으로 묶기 (non-overlap, label=마지막 행) ---\n",
    "group_size = 8\n",
    "step = 8  # non-overlapping\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. 데이터 전처리 (CSI + TOA) - Anchor ID 제거\n",
    "# 8행으로 나누어 떨어지지 않는 나머지 부분이 있다면 제거\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 학습 데이터 준비\n",
    "Xv_csi = df_train[csi_cols].values\n",
    "Xv_toa = df_train['TOA'].values\n",
    "yv = df_train[output_cols].values\n",
    "\n",
    "n_full = (Xv_csi.shape[0] // step) * step\n",
    "Xv_csi_cut = Xv_csi[:n_full]\n",
    "Xv_toa_cut = Xv_toa[:n_full]\n",
    "yv_cut = yv[:n_full]\n",
    "\n",
    "n_groups = n_full // step\n",
    "\n",
    "# CSI Reshape: (n_groups, 8, 128, 1)\n",
    "X_groups_csi = Xv_csi_cut.reshape(n_groups, step, 128)\n",
    "X_groups_csi = X_groups_csi[..., np.newaxis] # 4D 텐서로 변환\n",
    "\n",
    "# TOA Reshape: (n_groups, 8)\n",
    "X_groups_toa = Xv_toa_cut.reshape(n_groups, step)\n",
    "\n",
    "# Label: 그룹의 마지막 행 좌표 사용\n",
    "y_group_labels = yv_cut.reshape(n_groups, step, 2)[:, -1, :]\n",
    "\n",
    "# 테스트 데이터 준비\n",
    "Xv_test_csi = df_test[csi_cols].values\n",
    "Xv_test_toa = df_test['TOA'].values\n",
    "yv_test = df_test[output_cols].values\n",
    "\n",
    "n_full_test = (Xv_test_csi.shape[0] // step) * step\n",
    "Xv_test_csi_cut = Xv_test_csi[:n_full_test]\n",
    "Xv_test_toa_cut = Xv_test_toa[:n_full_test]\n",
    "yv_test_cut = yv_test[:n_full_test]\n",
    "\n",
    "n_groups_test = n_full_test // step\n",
    "\n",
    "X_groups_test_csi = Xv_test_csi_cut.reshape(n_groups_test, step, 128)[..., np.newaxis]\n",
    "X_groups_test_toa = Xv_test_toa_cut.reshape(n_groups_test, step)\n",
    "y_groups_test = yv_test_cut.reshape(n_groups_test, step, 2)[:, -1, :]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. 정규화 (Scaling)\n",
    "# ---------------------------------------------------------\n",
    "# 변수명 매핑 (User snippet compatibility)\n",
    "X_train_csi = X_groups_csi\n",
    "X_test_csi = X_groups_test_csi\n",
    "X_train_toa = X_groups_toa\n",
    "X_test_toa = X_groups_test_toa\n",
    "\n",
    "# [User Request] CSI Log1p + Max Scaling\n",
    "X_train_csi = np.log1p(X_train_csi)\n",
    "X_test_csi = np.log1p(X_test_csi)\n",
    "\n",
    "max_val = np.max(np.abs(X_train_csi))\n",
    "if max_val == 0: max_val = 1.0\n",
    "\n",
    "X_train_csi_scaled = X_train_csi / max_val\n",
    "X_test_csi_scaled = X_test_csi / max_val\n",
    "\n",
    "\n",
    "# axis=1 (step=8) 기준 평균/표준편차\n",
    "toa_mean_tr = np.mean(X_train_toa, axis=1, keepdims=True)\n",
    "toa_std_tr = np.std(X_train_toa, axis=1, keepdims=True)\n",
    "toa_std_tr[toa_std_tr == 0] = 1.0\n",
    "X_train_toa_scaled = (X_train_toa - toa_mean_tr) / toa_std_tr\n",
    "\n",
    "toa_mean_te = np.mean(X_test_toa, axis=1, keepdims=True)\n",
    "toa_std_te = np.std(X_test_toa, axis=1, keepdims=True)\n",
    "toa_std_te[toa_std_te == 0] = 1.0\n",
    "X_test_toa_scaled = (X_test_toa - toa_mean_te) / toa_std_te\n",
    "\n",
    "\n",
    "# Label 정규화\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_group_labels)\n",
    "y_test_scaled = scaler_y.transform(y_groups_test)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Multi-Input 모델 구축 (CSI + TOA)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Branch 1: CSI (CNN)\n",
    "input_csi = Input(shape=(8, 128, 1), name='input_csi')\n",
    "\n",
    "# [Step 1: 256으로 뻥튀기]\n",
    "x = Conv2D(256, (3, 3), padding='same')(input_csi)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# [Step 2: 128로 축소]\n",
    "x = Conv2D(128, (3, 3), padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# [Step 3: 64로 축소]\n",
    "x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(128)(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Branch 2: TOA (Dense) - 거리/지연 정보 학습\n",
    "input_toa = Input(shape=(8,), name='input_toa')\n",
    "y = Dense(32)(input_toa)\n",
    "y = LeakyReLU(alpha=0.1)(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Dense(16)(y)\n",
    "y = LeakyReLU(alpha=0.1)(y)\n",
    "\n",
    "# Concatenate (CSI + TOA)\n",
    "combined = concatenate([x, y])\n",
    "\n",
    "# Joint processing\n",
    "w = Dense(128)(combined)\n",
    "w = LeakyReLU(alpha=0.1)(w)\n",
    "w = Dropout(0.3)(w)\n",
    "\n",
    "output = Dense(2, name='output')(w) # 출력층은 좌표값이므로 활성화 함수 없이 Linear 유지\n",
    "\n",
    "model = Model(inputs=[input_csi, input_toa], outputs=output)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "model.summary()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. 모델 학습\n",
    "# ---------------------------------------------------------\n",
    "history = model.fit(\n",
    "    [X_train_csi_scaled, X_train_toa_scaled], \n",
    "    y_train_scaled, \n",
    "    epochs=40, \n",
    "    batch_size=128, \n",
    "    validation_split=0.2, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. 평가 및 예측\n",
    "# ---------------------------------------------------------\n",
    "loss, mae = model.evaluate([X_test_csi_scaled, X_test_toa_scaled], y_test_scaled, verbose=0)\n",
    "print(f'Test Loss: {loss:.4f}, Test MAE: {mae:.4f}')\n",
    "\n",
    "predictions = model.predict([X_test_csi_scaled, X_test_toa_scaled])\n",
    "predictions_original = scaler_y.inverse_transform(predictions)\n",
    "actual_original = scaler_y.inverse_transform(y_test_scaled)\n",
    "\n",
    "# RMSE 계산\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(actual_original, predictions_original))\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "\n",
    "# 모델 저장\n",
    "model.save('Final_model.h5')\n",
    "print(\"모델이 'Final_model.h5'로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14deeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 10. CDF of Positioning Errors\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 오차 계산 (유클리드 거리)\n",
    "errors = np.sqrt((actual_original[:, 0] - predictions_original[:, 0])**2 + (actual_original[:, 1] - predictions_original[:, 1])**2)\n",
    "\n",
    "# CDF 계산\n",
    "sorted_errors = np.sort(errors)\n",
    "cdf = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "\n",
    "# 플롯\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(sorted_errors, cdf, label='CDF of Errors')\n",
    "plt.xlabel('Positioning Error (m)')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('CDF of Positioning Errors')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 추가 통계 출력\n",
    "print(f\"Mean Error: {np.mean(errors):.4f} m\")\n",
    "print(f\"Median Error: {np.median(errors):.4f} m\")\n",
    "print(f\"90th Percentile Error: {np.percentile(errors, 90):.4f} m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobility_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
